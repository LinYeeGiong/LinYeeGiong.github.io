<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>自监督学习</title>
    <link href="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="自监督学习（self-supervised-Learning）"><a href="#自监督学习（self-supervised-Learning）" class="headerlink" title="自监督学习（self-supervised Learning）"></a>自监督学习（self-supervised Learning）</h2><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-1.png" class="" title="自监督学习"><ul><li>定义：Predict parts of input data from other parts, using the data’s inherent structure instead of explicit labels</li><li>分类：<ul><li>基于前置任务</li><li>基于对比学习</li><li>基于掩码重建</li></ul></li></ul><h3 id="1-基于前置任务"><a href="#1-基于前置任务" class="headerlink" title="1. 基于前置任务"></a>1. 基于前置任务</h3><blockquote><p>设计一个任务（比如说位置预测和拼图），让网络去学习图像中本身就存在的位置关系，这样标签label就可以从原始数据本身当中创造。</p></blockquote><ul><li>a.位置预测<ul><li>Relative positioning：把图分成不同的patch，用两个相同的CNN，分别输入中心patch和1~9的其中一个patch，让网络去预测这个patch是在中心patch的哪个方位上或者说是预测这个patch是几号。</li><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-2.png" class="" title="自监督学习"></li><li>Solving the jigsaw：把原始图像分patch，然后打乱patch的顺序，让CFN网络去预测正确的顺序</li><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-3.png" class="" title="自监督学习"></li></ul></li><li>b.旋转预测<ul><li>将图像旋转，让模型去预测旋转角度，这样模型就能学习到一些比如说鼻子嘴巴眼睛的特征？</li><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-4.png" class="" title="自监督学习"></li></ul></li><li>c.上色（Colorization）<ul><li>要求网络对一张灰度图的每个像素去上色还原成原始RGB图像。因为是像素级的操作，因此细腻度更高，适合分割任务</li><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-5.png" class="" title="自监督学习"></li></ul></li><li>d.聚类预测<ul><li>使用聚类算法生成假标签，然后用假标签去训练网络反向传播，然后再根据更新后的CNN输出再去做聚类，再用生成的假标签去训练，如此反复</li><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-6.png" class="" title="自监督学习"></li></ul></li></ul><h3 id="2-基于对比学习"><a href="#2-基于对比学习" class="headerlink" title="2. 基于对比学习"></a>2. 基于对比学习</h3><blockquote><p>将一张图像通过数据增强后，通过约束Loss，让正样本间的距离尽可能近，负样本间的距离尽可能远</p></blockquote><p>$\begin{aligned} \mathcal{L}<em>{\mathrm{infoNCE}} &amp; &#x3D;-\log\frac{\exp(\sin(z_i,z^+)&#x2F;\tau)}{\exp(\sin(z_i,z^+)&#x2F;\tau)+\sum</em>{j&#x3D;1}^k\exp\left(\sin\left(z_i,z_j^-\right)&#x2F;\tau\right)}&#x3D;-\log\frac{\exp(\sin(z_i,z^+)&#x2F;\tau)}{\sum_{j&#x3D;1}^k\exp(\sin(z_i,z_j)&#x2F;\tau)} \ &amp; sim\left(z_i,z_j\right)&#x3D;\frac{z_i.z_j}{\parallel z_i\parallel.\parallel z_j\parallel} \end{aligned}$</p><ul><li>a.SimCLR<ul><li><img src="/2025/11/13/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/images-7.png" class="" title="自监督学习"></li><li>这里在下游任务中，使用的是没经过MLP的原始特征，因为MLP的作用就是让$h_i$和$h_j$更加接近，从而更好的去训练，但是明显经过增强后的两种图像应该是有比较明显的差异的，因此$h_i$和$h_j$会蕴含更多的特征信息</li><li>随机裁剪和颜色抖动能取得较好效果</li><li>较大的batch-size也可以获得比较好的效果</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>网课学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MLLMs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM-1.大模型综述</title>
    <link href="/2025/11/11/LLM-1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/"/>
    <url>/2025/11/11/LLM-1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1、基于大模型对话的系统架构，宏观视角"><a href="#1、基于大模型对话的系统架构，宏观视角" class="headerlink" title="1、基于大模型对话的系统架构，宏观视角"></a>1、基于大模型对话的系统架构，宏观视角</h2><img src="/2025/11/11/LLM-1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/image-20251111232522172.png" class="" title="LLM-1-大模型综述"><blockquote><ul><li>一条路径是Prompt工程</li><li>另一条路径是微调</li></ul></blockquote><h2 id="2、大模型发展的四个阶段"><a href="#2、大模型发展的四个阶段" class="headerlink" title="2、大模型发展的四个阶段"></a>2、大模型发展的四个阶段</h2><ul><li>非神经网络时代的完全监督学习<ul><li>逻辑回归、支持向量机、朴素贝叶斯</li></ul></li><li>基于神经网络的完全监督学习<ul><li>word2vec</li></ul></li><li>预训练，精调范式（Pre-train，Fine-tune）<ul><li>Bert、GPT、Bart</li></ul></li><li>预训练，提示、预测范式（Pre-train，Prompt， Predict）也叫零样本学习Zero-shot<ul><li>GPT2、GPT3、羊驼、ChatGlm</li></ul></li></ul><blockquote><p>Fine-tuning和Prompting的区别</p><ul><li>Fine-tuning是改变模型去契合任务</li><li>Prompting是任务+提示词来让模型有正确的输出；不改变模型本身，模型本身足够强大，所以也称为AGI通用人工智能</li></ul></blockquote><table><thead><tr><th>范式</th><th>第1、2种范式：完全监督学习 (Fully Supervised Learning；非神经网络和神经网络)</th><th>第3种范式：预训练-微调</th><th>第4种范式：预训练-提示学习</th></tr></thead><tbody><tr><td><strong>训练数据</strong></td><td>目标任务数据集</td><td>大规模生语料、目标任务数据集</td><td>大规模生语料、目标任务数据集</td></tr><tr><td><strong>输入</strong></td><td>我是谁？</td><td>我是谁？</td><td>[CLS] 我是谁？ [SEP] <br>[MASK] 我是 [MASK] [SEP]</td></tr><tr><td><strong>输出</strong></td><td>[0,0,1]</td><td>[0,0,1]</td><td>[CLS] 哲学 [SEP]</td></tr><tr><td><strong>输出层</strong></td><td>一个线性变换</td><td>一个线性变换</td><td>无新增结构</td></tr><tr><td><strong>特点</strong></td><td>依赖目标任务数据集来获得文本表示。</td><td>基于庞大的生语料获取良好的文本表示；再由目标任务数据集获得下游任务知识。</td><td>基于庞大的生语料获取良好的文本表示；通过提示（模板）构造输入以增强任务迁移与生成能力，并与下游任务无缝衔接。</td></tr></tbody></table><h2 id="3、Prompt工程"><a href="#3、Prompt工程" class="headerlink" title="3、Prompt工程"></a>3、Prompt工程</h2><p>高级Prompti提示</p><ul><li>零样本提示（Zero-shot Prompting）</li><li>少样本提示（Few-shot Prompting）</li><li>思维链CoT提示（Chain-of-Thought Prompting）</li><li>零样本CoT（Zero-shot CoT）</li><li>自一致性（Self-Consistency）</li><li>生成知识提示（Generate Knowledge Prompting）</li><li>自动提示工程（Automatic Prompt Engineer）</li></ul><h2 id="4、大模型的内核：Transformer"><a href="#4、大模型的内核：Transformer" class="headerlink" title="4、大模型的内核：Transformer"></a>4、大模型的内核：Transformer</h2><table><thead><tr><th>模型</th><th>结构</th><th>位置编码</th><th>激活函数</th><th>layer norm 方法</th></tr></thead><tbody><tr><td>原生 Transformer</td><td>Encoder-Decoder</td><td>Sinusoidal 编码</td><td>ReLU</td><td>Post layer norm</td></tr><tr><td>BERT</td><td>Encoder</td><td>绝对位置编码</td><td>GeLU</td><td>Post layer norm</td></tr><tr><td>LLaMA</td><td>Casual decoder</td><td>RoPE</td><td>SwiGLU</td><td>Pre RMS Norm</td></tr><tr><td>ChatGLM-6B</td><td>Prefix decoder</td><td>RoPE</td><td>GeGLU</td><td>Post Deep Norm</td></tr><tr><td>Bloom</td><td>Casual decoder</td><td>ALiBi</td><td>GeLU</td><td>Pre Layer Norm</td></tr></tbody></table><h3 id="4-1-大模型的架构"><a href="#4-1-大模型的架构" class="headerlink" title="4.1 大模型的架构"></a>4.1 大模型的架构</h3><img src="/2025/11/11/LLM-1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/image-20251111235817641.png" class="" title="LLM-1-大模型综述"><ul><li><p>Encoder-Decoder（BART、T5）：Encoder阶段前后都能看到，Decoder只能向前看</p></li><li><p>Decoder（GPT、LLAMa）</p></li><li><p>Prefix LM（GLM模型）</p></li></ul><h3 id="4-2-激活函数"><a href="#4-2-激活函数" class="headerlink" title="4.2 激活函数"></a>4.2 激活函数</h3><p>ReLU太硬了，使用正态分布的概率密度函数来做，然后用函数近似，得到GeLU，然后再衍生出变体。</p><h3 id="4-3-位置编码"><a href="#4-3-位置编码" class="headerlink" title="4.3 位置编码"></a>4.3 位置编码</h3><blockquote><p>在NLP任务中，”我借你钱“和”你借我钱“是意思完全不同的两句话，所以词的位置变化有影响，因此需要做位置编码来保持对词位置的敏感</p><ul><li>但其实有的时候其实对绝对位置不敏感，对相对位置比较敏感（比如考上大学，其实不是考了多少分，而是在省内的排名，省内排名就是相对位置，有多少人分数比你高，有多少人分数比你低）</li><li>因此在这里更关心的是这个词离我有多远，远到什么程度，而相隔越远的词相似度可能越低</li></ul></blockquote><h2 id="需要复习一下大模型的结构"><a href="#需要复习一下大模型的结构" class="headerlink" title="需要复习一下大模型的结构"></a>需要复习一下大模型的结构</h2>]]></content>
    
    
    <categories>
      
      <category>网课学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>追风的第一篇博客</title>
    <link href="/2025/11/10/%E8%BF%BD%E9%A3%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2025/11/10/%E8%BF%BD%E9%A3%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>这是我的第一篇博客，起因是感慨研一上已经过去两个月了，貌似还是无所事事，于是想着来记录点什么，留下点什么，所以在应用信息论这节晚课上，依托hexo与github搭建了这个树洞，希望接下来能好好记录，沉淀下来！</p>]]></content>
    
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/11/10/hello-world/"/>
    <url>/2025/11/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
